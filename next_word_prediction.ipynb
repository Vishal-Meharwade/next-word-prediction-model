{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s9EhTzFpS7V",
        "outputId": "0e85efee-cf3c-40c5-d649-6c28939ce2c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n",
            "Downloading dataset...\n",
            "--2024-12-15 14:10:06--  https://raw.githubusercontent.com/dscape/spell/master/test/resources/big.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6488666 (6.2M) [text/plain]\n",
            "Saving to: ‘dataset.txt’\n",
            "\n",
            "dataset.txt         100%[===================>]   6.19M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-12-15 14:10:06 (227 MB/s) - ‘dataset.txt’ saved [6488666/6488666]\n",
            "\n",
            "Epoch 1/10\n",
            "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.0379 - loss: 7.4988\n",
            "Epoch 1: loss improved from inf to 7.08158, saving model to /content/best_model.keras\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.0381 - loss: 7.4794 - val_accuracy: 0.0240 - val_loss: 6.3911\n",
            "Epoch 2/10\n",
            "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.0353 - loss: 6.1020\n",
            "Epoch 2: loss improved from 7.08158 to 6.10702, saving model to /content/best_model.keras\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.0358 - loss: 6.1022 - val_accuracy: 0.0405 - val_loss: 6.4312\n",
            "Epoch 3/10\n",
            "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.0549 - loss: 5.9185\n",
            "Epoch 3: loss improved from 6.10702 to 5.94209, saving model to /content/best_model.keras\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.0549 - loss: 5.9196 - val_accuracy: 0.0405 - val_loss: 6.4712\n",
            "Epoch 4/10\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0542 - loss: 5.8099\n",
            "Epoch 4: loss improved from 5.94209 to 5.83704, saving model to /content/best_model.keras\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 69ms/step - accuracy: 0.0543 - loss: 5.8105 - val_accuracy: 0.0405 - val_loss: 6.5373\n",
            "Epoch 5/10\n",
            "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.0564 - loss: 5.8062\n",
            "Epoch 5: loss improved from 5.83704 to 5.74517, saving model to /content/best_model.keras\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - accuracy: 0.0563 - loss: 5.8034 - val_accuracy: 0.0405 - val_loss: 6.6663\n",
            "Epoch 6/10\n",
            "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.0578 - loss: 5.6532\n",
            "Epoch 6: loss improved from 5.74517 to 5.67141, saving model to /content/best_model.keras\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.0576 - loss: 5.6540 - val_accuracy: 0.0405 - val_loss: 6.6684\n",
            "Epoch 7/10\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.0574 - loss: 5.6041\n",
            "Epoch 7: loss improved from 5.67141 to 5.60944, saving model to /content/best_model.keras\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.0573 - loss: 5.6042 - val_accuracy: 0.0405 - val_loss: 6.8650\n",
            "Epoch 8/10\n",
            "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.0599 - loss: 5.5118\n",
            "Epoch 8: loss improved from 5.60944 to 5.55615, saving model to /content/best_model.keras\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.0595 - loss: 5.5139 - val_accuracy: 0.0405 - val_loss: 6.9678\n",
            "Epoch 9/10\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.0515 - loss: 5.5280\n",
            "Epoch 9: loss improved from 5.55615 to 5.48897, saving model to /content/best_model.keras\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.0516 - loss: 5.5271 - val_accuracy: 0.0405 - val_loss: 7.0549\n",
            "Epoch 10/10\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.0570 - loss: 5.3969\n",
            "Epoch 10: loss improved from 5.48897 to 5.41265, saving model to /content/best_model.keras\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.0570 - loss: 5.3973 - val_accuracy: 0.0405 - val_loss: 7.1774\n",
            "\n",
            "Prediction Examples:\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
            "Seed: the quick brown | Predicted Next Word: the\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Seed: machine learning is | Predicted Next Word: the\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Seed: artificial intelligence will | Predicted Next Word: the\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import gc\n",
        "\n",
        "# Colab-specific memory management\n",
        "def reduce_memory_usage():\n",
        "    gc.collect()\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "# Download and preprocess dataset\n",
        "def download_dataset():\n",
        "    print(\"Downloading dataset...\")\n",
        "    !wget -O dataset.txt https://raw.githubusercontent.com/dscape/spell/master/test/resources/big.txt\n",
        "    return 'dataset.txt'\n",
        "\n",
        "# Read and preprocess text\n",
        "def preprocess_text(file_path, max_words=10000):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read().lower()\n",
        "        text = ''.join([char for char in text if char.isalpha() or char.isspace()])\n",
        "\n",
        "    # Limit text to prevent memory issues\n",
        "    words = text.split()[:max_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Memory-efficient sequence generation\n",
        "def generate_sequences(tokenizer, text, max_sequence_len=20, step=3):\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    sequences = []\n",
        "    next_words = []\n",
        "\n",
        "    for i in range(0, len(token_list) - max_sequence_len, step):\n",
        "        sequence = token_list[i:i + max_sequence_len]\n",
        "        sequences.append(sequence[:-1])\n",
        "        next_words.append(sequence[-1])\n",
        "\n",
        "    return sequences, next_words\n",
        "\n",
        "# Colab-friendly model with reduced complexity\n",
        "def create_model(vocab_size, max_sequence_len):\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, 64, input_length=max_sequence_len-1),\n",
        "        LSTM(128, return_sequences=False),\n",
        "        Dropout(0.2),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Main training function\n",
        "def train_next_word_predictor():\n",
        "    # Reduce memory usage at start\n",
        "    reduce_memory_usage()\n",
        "\n",
        "    # Download and preprocess dataset\n",
        "    dataset_path = download_dataset()\n",
        "    text = preprocess_text(dataset_path)\n",
        "\n",
        "    # Tokenization\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts([text])\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "    # Sequence generation\n",
        "    max_sequence_len = 20\n",
        "    X_seq, y_seq = generate_sequences(tokenizer, text, max_sequence_len)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X = np.array(X_seq)\n",
        "    y = np.array(y_seq)\n",
        "\n",
        "    # Create model\n",
        "    model = create_model(vocab_size, max_sequence_len)\n",
        "\n",
        "    # Checkpoint to save best model\n",
        "    checkpoint_path = \"/content/best_model.keras\"\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        checkpoint_path,\n",
        "        monitor='loss',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Training with memory-efficient approach\n",
        "    try:\n",
        "        history = model.fit(\n",
        "            X, y,\n",
        "            epochs=10,\n",
        "            batch_size=64,\n",
        "            validation_split=0.2,\n",
        "            callbacks=[checkpoint],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Save final model\n",
        "        model.save(\"/content/final_next_word_model.keras\")\n",
        "\n",
        "        return model, tokenizer, vocab_size\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Training error: {e}\")\n",
        "        return None, None, None\n",
        "    finally:\n",
        "        reduce_memory_usage()\n",
        "\n",
        "# Prediction function\n",
        "def predict_next_word(seed_text, model, tokenizer, vocab_size, max_sequence_len=20):\n",
        "    # Tokenize and pad input sequence\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\n",
        "    # Predict next word\n",
        "    predicted = model.predict(token_list)\n",
        "    predicted_word_index = np.argmax(predicted, axis=-1)[0]\n",
        "\n",
        "    # Convert index back to word\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted_word_index:\n",
        "            return word\n",
        "\n",
        "    return \"Unable to predict\"\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Train the model\n",
        "    print(\"Starting model training...\")\n",
        "    model, tokenizer, vocab_size = train_next_word_predictor()\n",
        "\n",
        "    if model and tokenizer:\n",
        "        # Test prediction\n",
        "        test_seeds = [\n",
        "            \"the quick brown\",\n",
        "            \"machine learning is\",\n",
        "            \"artificial intelligence will\"\n",
        "        ]\n",
        "\n",
        "        print(\"\\nPrediction Examples:\")\n",
        "        for seed in test_seeds:\n",
        "            predicted_word = predict_next_word(seed, model, tokenizer, vocab_size)\n",
        "            print(f\"Seed: {seed} | Predicted Next Word: {predicted_word}\")\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z2Cm1vdApT96"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}